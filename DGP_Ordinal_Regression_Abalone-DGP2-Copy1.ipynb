{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZErHJxBry8n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12918200221195333655\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7905922253\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2060363817790192070\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "The installed version of TensorFlow includes GPU support.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow does not include GPU support.\")\n",
    "    \n",
    "tf.logging.set_verbosity(0)\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "plt = matplotlib.pyplot\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from ordinal import Ordinal\n",
    "from gpflow.kernels import RBF, White\n",
    "from gpflow.models.gpr import GPR\n",
    "from gpflow.models.svgp import SVGP\n",
    "from gpflow.training import AdamOptimizer, ScipyOptimizer\n",
    "import gpflow.training.monitor as mon\n",
    "from gpflow.kernels import Matern32\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "import gpflow\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryrUYcL3-t8t"
   },
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iwoYRMpR23w"
   },
   "outputs": [],
   "source": [
    "def make_dgp_models(X, Y, Z,likelihood,layers):\n",
    "    models, names = [], []\n",
    "    for L in layers:\n",
    "        D = X.shape[1]\n",
    "        N = X.shape[0]\n",
    "        print('Design Matrix Shape:'+str(D)+'X'+str(N))\n",
    "        \n",
    "        # the layer shapes are defined by the kernel dims, so here all hidden layers are D dimensional \n",
    "        kernels = []\n",
    "        for l in range(L):\n",
    "            kernels.append(RBF(D,variance = 1.5,lengthscales=0.5,ARD=True))\n",
    "\n",
    "        # between layer noise (doesn't actually make much difference but we include it anyway)\n",
    "        for kernel in kernels[:-1]:\n",
    "            white = White(D, variance=1e-5)\n",
    "#             tf.is_variable_initialized(white.variance).mark_used()\n",
    "            kernel += white\n",
    "\n",
    "        mb = 1000 if X.shape[0] > 1000 else None \n",
    "        model = DGP(X, Y, Z, kernels, likelihood, num_samples=10, minibatch_size=mb)\n",
    "\n",
    "        # start the inner layers almost deterministically \n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.q_sqrt = layer.q_sqrt.value * 1e-5\n",
    "\n",
    "        models.append(model)\n",
    "        names.append('DGP{} {}'.format(L, len(Z)))\n",
    "        model.compile()\n",
    "    \n",
    "    return models, names\n",
    "\n",
    "def make_single_layer_model(X,Y,Z,likelihood):\n",
    "    D = X.shape[1]\n",
    "    m_svgp = SVGP(X, Y, RBF(D,variance = 1.5,lengthscales=0.5,ARD=True), likelihood, Z.copy())\n",
    "    name = 'SVGP'+str(len(Z))\n",
    "    return m_svgp, name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md3egkBqOChk"
   },
   "source": [
    "**Prediction**<br>\n",
    "We'll calculate test absolute error in batches (so the larger datasets don't cause memory problems)\n",
    "\n",
    "For the DGP models we need to take an average over the samples for the absolute error. The predict_density function already does this internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DueT172JOBQy"
   },
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/1000.), 1)\n",
    "    AbsErr = []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "        abserr = assess_model(model, X_batch, Y_batch)\n",
    "        AbsErr.append(abserr)\n",
    "        \n",
    "    MAErr = np.average(np.concatenate(AbsErr, 0))\n",
    "    \n",
    "    return MAErr\n",
    "\n",
    "def assess_single_layer(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch)\n",
    "    mean = np.average(m, 0)\n",
    "    abserr = np.absolute((mean - Y_batch))\n",
    "    return abserr\n",
    "S = 100\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch, S)\n",
    "    \n",
    "    mean = np.average(m, 0)\n",
    "    abserr = np.absolute((mean - Y_batch))\n",
    "    return abserr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_RJIbNqRJp8"
   },
   "outputs": [],
   "source": [
    "iterations_few = 100\n",
    "iterations_many = 5000\n",
    "s = '{:<16}  Mean Absolute Error : {:.4f} +- {:.4f}'\n",
    "iterations = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "class CustomTestPerformanceTensorBoardTask(mon.BaseTensorBoardTask):\n",
    "    def __init__(self, file_writer, model, Xt, Yt,S = 100):\n",
    "        super().__init__(file_writer, model)\n",
    "        self.Xt = Xt\n",
    "        self.Yt = Yt\n",
    "        self.Y_std = Y_std\n",
    "        self._full_test_err = tf.placeholder(gpflow.settings.float_type, shape=())\n",
    "        self._full_test_nlpp = tf.placeholder(gpflow.settings.float_type, shape=())\n",
    "        self._summary = tf.summary.merge([tf.summary.scalar(model.name +\"/test_rmse\", self._full_test_err),\n",
    "                                         tf.summary.scalar(model.name +\"/test_nlpp\", self._full_test_nlpp)])\n",
    "        self.S = S\n",
    "    \n",
    "    def batch_assess(self,assess_model, X, Y,minibatch_size = 1000.):\n",
    "        n_batches = max(int(X.shape[0]/minibatch_size), 1)\n",
    "        AbsErr = []\n",
    "        for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "            abserr = assess_model(X_batch, Y_batch)\n",
    "            AbsErr.append(abserr)\n",
    "\n",
    "        MAErr = np.average(np.concatenate(AbsErr, 0))\n",
    "\n",
    "        return MAErr\n",
    "      \n",
    "\n",
    "    def assess_sampled(self, X_batch, Y_batch):\n",
    "        m, v = self.model.predict_y(X_batch, self.S)\n",
    "\n",
    "        mean = np.average(m, 0)\n",
    "        abserr = np.absolute((mean - Y_batch))\n",
    "        return abserr\n",
    "    \n",
    "       \n",
    "    \n",
    "    def run(self, context: mon.MonitorContext, *args, **kwargs) -> None:        \n",
    "        test_mean_abs_err = self.batch_assess(self.assess_sampled, self.Xt, self.Yt)            \n",
    "        self._eval_summary(context, {self._full_test__mean_abs_err: test_mean_abs_err})\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPrintTask(mon.MonitorTask):\n",
    "    def __init__(self,  model): \n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def run(self, context: mon.MonitorContext, *args, **kwargs) -> None:        \n",
    "        print(self.model.likelihood.likelihood.bin_start)\n",
    "        print(self.model.likelihood.likelihood.bin_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2740
    },
    "colab_type": "code",
    "id": "xJA8W53YDQpJ",
    "outputId": "ba5760f5-957c-427b-86f2-b2f3792b3e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n",
      "Design Matrix Shape:10X1000\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "testString =  \"./abalone_dataset/10bins/abalone_test_10.\"\n",
    "trainString = \"./abalone_dataset/10bins/abalone_train_10.\"\n",
    "\n",
    "MAErr=[]\n",
    "\n",
    "for j in range(1,21):\n",
    "    flag = True\n",
    "    dfTest = pd.read_csv(testString + str(j),header = None,sep = ' ')\n",
    "    dfTrain = pd.read_csv(trainString +  str(j),header = None,sep = ' ')\n",
    "    train= np.array(dfTrain)\n",
    "    Y = train[:,-1:]\n",
    "    X = train[:, :-1]\n",
    "    test  = np.array(dfTest)\n",
    "    YT = test[:,-1:]\n",
    "    XT = test[:, :-1]\n",
    "    YT = YT -1\n",
    "    Y = Y -1\n",
    "#     bin_edges = []\n",
    "#     uniqueY = np.unique(Y)\n",
    "#     for i in range(uniqueY.size - 1 ):\n",
    "#         bin_edges.append((uniqueY[i] + uniqueY[i+1])/2)\n",
    "#     bin_edges = np.array(bin_edges)    \n",
    "    \n",
    "    # construct ordinal likelihood - bin_edges is the same as unique(Y) but centered\n",
    "#     bin_edges = np.array(np.arange(np.unique(Y).size - 1), dtype=float)\n",
    "#     bin_edges = bin_edges - bin_edges.mean()\n",
    "    \n",
    "    # bin_edges = bin_edges - bin_edges.mean()\n",
    "    likelihood= Ordinal(bin_start=0, bin_width = 2, num_edges = np.unique(Y).size - 1)\n",
    "    Z = kmeans2(X, 100, minit='points')[0]\n",
    "    models_dgp, names_dgp = make_dgp_models(X, Y, Z,likelihood,[2])\n",
    "    maerr=[]\n",
    "    for m, name in zip(models_dgp, names_dgp):\n",
    "        \n",
    "#         print_task = CustomPrintTask(m).with_name('print_tboard')\\\n",
    "#             .with_condition(mon.PeriodicIterationCondition(1))\\\n",
    "#             .with_exit_condition(True)\n",
    "#         monitor_tasks = [print_task]\n",
    "#         session = m.enquire_session()\n",
    "#         global_step = mon.create_global_step(session)\n",
    "        \n",
    "#         opti miser = AdamOptimizer(0.01)\n",
    "        \n",
    "#         with mon.Monitor(monitor_tasks, session,global_step, print_summary=True) as monitor:\n",
    "#             optimiser.minimize(m, step_callback=monitor, maxiter=iterations, global_step=global_step)\n",
    "       \n",
    "        AdamOptimizer(0.01).minimize(m, maxiter=iterations)\n",
    "        maerr.append(batch_assess(m, assess_sampled, XT, YT))\n",
    "    model_svgp, svgp_name = make_single_layer_model(X,Y,Z,likelihood)\n",
    "    ScipyOptimizer().minimize(model_svgp, maxiter=iterations)\n",
    "    maerr.append(batch_assess(model_svgp, assess_single_layer, XT, YT))\n",
    "    \n",
    "    MAErr.append(maerr)\n",
    "\n",
    "MAErr=np.array(MAErr)    \n",
    "\n",
    "\n",
    "  \n",
    "# tempInc = 2\n",
    "# temp1 = 320000 iterations\n",
    "\n",
    "# temp2 = 0\n",
    "# i =0 \n",
    "# #     while tempInc > .00001:\n",
    "# #         if(flag):\n",
    "# AdamOptimizer(0.01).minimize(m,maxiter=4000)\n",
    "# #         flag = false\n",
    "# #         AdamOptimizer(0.01).minimize(m,maxiter=500)\n",
    "# #     ScipyOptimizer().minimize(m,maxiter=4000)\n",
    "# Xs = XT\n",
    "# S = 10\n",
    "# mean,var= m.predict_y(Xs,S)\n",
    "# fmean = np.average(mean,0)\n",
    "# fvar = np.average(var,0)\n",
    "# #     temp2 = np.average(np.absolute((np.round(fmean) - YT)))\n",
    "# #     tempInc = temp1 - temp2\n",
    "# #     temp1 = temp2\n",
    "# print(testString + str(j))\n",
    "# print(np.average(np.absolute((np.round(fmean) - YT))))\n",
    "# #     i = i +1 \n",
    "# #     print(i )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULSTXFWGCacj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGP2 100          Mean Absolute Error : 0.5761 +- 0.0087\n",
      "SVGP100           Mean Absolute Error : 0.5614 +- 0.0076\n"
     ]
    }
   ],
   "source": [
    "meanMAErr = np.mean(MAErr,0)\n",
    "stdMAErr = np.std(MAErr,0)\n",
    "\n",
    "for i in range(len(names_dgp)):\n",
    "    print(s.format(names_dgp[i],meanMAErr[i],stdMAErr[i]))\n",
    "    \n",
    "print(s.format(svgp_name,meanMAErr[-1],stdMAErr[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var 1.5 l 0.5\n",
    "DGP2 100          Mean Absolute Error : 0.6173 +- 0.0067\n",
    "SVGP100           Mean Absolute Error : 0.5735 +- 0.0036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var 0.5 l 0.5\n",
    "DGP2 100          Mean Absolute Error : 0.6338 +- 0.0097\n",
    "SVGP100           Mean Absolute Error : 0.5694 +- 0.0009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-2e078b4bb02f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-2e078b4bb02f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    var 2 l 0.5\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "var 2 l 0.5\n",
    "DGP2 100          Mean Absolute Error : 0.6162 +- 0.0019\n",
    "SVGP100           Mean Absolute Error : 0.5743 +- 0.0037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var 2 l 2\n",
    "DGP2 100          Mean Absolute Error : 1.6667 +- 0.0022\n",
    "SVGP100           Mean Absolute Error : 0.5716 +- 0.0002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var 2 l= 1.0\n",
    "DGP2 100          Mean Absolute Error : 0.6573 +- 0.0005\n",
    "SVGP100           Mean Absolute Error : 0.5727 +- 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dgp[0].likelihood.likelihood.bin_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dgp[0].likelihood.likelihood.bin_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20000 iterations\n",
    "DGP2 100          Mean Absolute Error : 0.5743 +- 0.0088\n",
    "SVGP100           Mean Absolute Error : 0.5634 +- 0.0075"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DGP_Ordinal_Regression_Abalone.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
