{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZErHJxBry8n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18177090429243490066\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7905922253\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16634289191896804325\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "The installed version of TensorFlow includes GPU support.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.is_built_with_cuda():\n",
    "    print(\"The installed version of TensorFlow includes GPU support.\")\n",
    "else:\n",
    "    print(\"The installed version of TensorFlow does not include GPU support.\")\n",
    "    \n",
    "tf.logging.set_verbosity(0)\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "%matplotlib inline\n",
    "\n",
    "plt = matplotlib.pyplot\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.likelihoods import Ordinal\n",
    "from gpflow.kernels import RBF, White\n",
    "from gpflow.models.gpr import GPR\n",
    "from gpflow.training import AdamOptimizer, ScipyOptimizer\n",
    "from gpflow.kernels import Matern32\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "import gpflow\n",
    "matplotlib.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryrUYcL3-t8t"
   },
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6iwoYRMpR23w"
   },
   "outputs": [],
   "source": [
    "def make_dgp_models(X, Y, Z,likelihood):\n",
    "    models, names = [], []\n",
    "    for L in range(1, 5):\n",
    "        D = X.shape[1]\n",
    "\n",
    "        # the layer shapes are defined by the kernel dims, so here all hidden layers are D dimensional \n",
    "        kernels = []\n",
    "        for l in range(L):\n",
    "            kernels.append(RBF(D,variance = 2.5,lengthscales=1.5))\n",
    "\n",
    "        # between layer noise (doesn't actually make much difference but we include it anyway)\n",
    "        for kernel in kernels[:-1]:\n",
    "            white = White(D, variance=1e-5)\n",
    "#             tf.is_variable_initialized(white.variance).mark_used()\n",
    "            kernel += white\n",
    "\n",
    "        mb = 1000 if X.shape[0] > 1000 else None \n",
    "        model = DGP(X, Y, Z, kernels, likelihood, num_samples=10, minibatch_size=mb)\n",
    "\n",
    "        # start the inner layers almost deterministically \n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.q_sqrt = layer.q_sqrt.value * 1e-5\n",
    "\n",
    "        models.append(model)\n",
    "        names.append('DGP{} {}'.format(L, len(Z)))\n",
    "        model.compile()\n",
    "    \n",
    "    return models, names\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Md3egkBqOChk"
   },
   "source": [
    "**Prediction**<br>\n",
    "We'll calculate test absolute error in batches (so the larger datasets don't cause memory problems)\n",
    "\n",
    "For the DGP models we need to take an average over the samples for the absolute error. The predict_density function already does this internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DueT172JOBQy"
   },
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/1000.), 1)\n",
    "    AbsErr = []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "        abserr = assess_model(model, X_batch, Y_batch)\n",
    "        AbsErr.append(abserr)\n",
    "        \n",
    "    MAErr = np.average(np.concatenate(AbsErr, 0))\n",
    "    \n",
    "    return MAErr\n",
    "\n",
    "S = 100\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch, S)\n",
    "    \n",
    "    mean = np.average(m, 0)\n",
    "    abserr = np.absolute((mean - Y_batch))\n",
    "    return abserr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H_RJIbNqRJp8"
   },
   "outputs": [],
   "source": [
    "iterations_few = 100\n",
    "iterations_many = 5000\n",
    "s = '{:<16}  Mean Absolute Error : {:.4f} +- {:.4f}'\n",
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2740
    },
    "colab_type": "code",
    "id": "xJA8W53YDQpJ",
    "outputId": "ba5760f5-957c-427b-86f2-b2f3792b3e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n",
      "10 10\n"
     ]
    }
   ],
   "source": [
    "testString =  \"./abalone_dataset/10bins/abalone_test_10.\"\n",
    "trainString = \"./abalone_dataset/10bins/abalone_train_10.\"\n",
    "\n",
    "MAErr=[]\n",
    "\n",
    "for j in range(1,21):\n",
    "    flag = True\n",
    "    dfTest = pd.read_csv(testString + str(j),header = None,sep = ' ')\n",
    "    dfTrain = pd.read_csv(trainString +  str(j),header = None,sep = ' ')\n",
    "    train= np.array(dfTrain)\n",
    "    Y = train[:,-1:]\n",
    "    X = train[:, :-1]\n",
    "    test  = np.array(dfTest)\n",
    "    YT = test[:,-1:]\n",
    "    XT = test[:, :-1]\n",
    "    YT = YT -1\n",
    "    Y = Y -1\n",
    "#     bin_edges = []\n",
    "#     uniqueY = np.unique(Y)\n",
    "#     for i in range(uniqueY.size - 1 ):\n",
    "#         bin_edges.append((uniqueY[i] + uniqueY[i+1])/2)\n",
    "#     bin_edges = np.array(bin_edges)    \n",
    "    \n",
    "    # construct ordinal likelihood - bin_edges is the same as unique(Y) but centered\n",
    "    bin_edges = np.array(np.arange(np.unique(Y).size - 1), dtype=float)\n",
    "    bin_edges = bin_edges - bin_edges.mean()\n",
    "    \n",
    "    # bin_edges = bin_edges - bin_edges.mean()\n",
    "    likelihood= Ordinal(bin_edges)\n",
    "    Z = kmeans2(X, 100, minit='points')[0]\n",
    "    models_dgp, names_dgp = make_dgp_models(X, Y, Z,likelihood)\n",
    "    maerr=[]\n",
    "    for m, name in zip(models_dgp, names_dgp):\n",
    "        AdamOptimizer(0.01).minimize(m, maxiter=iterations)\n",
    "        maerr.append(batch_assess(m, assess_sampled, XT, YT))\n",
    "    MAErr.append(maerr)\n",
    "\n",
    "MAErr=np.array(MAErr)    \n",
    "\n",
    "\n",
    "  \n",
    "# tempInc = 2\n",
    "# temp1 = 3\n",
    "# temp2 = 0\n",
    "# i =0 \n",
    "# #     while tempInc > .00001:\n",
    "# #         if(flag):\n",
    "# AdamOptimizer(0.01).minimize(m,maxiter=4000)\n",
    "# #         flag = false\n",
    "# #         AdamOptimizer(0.01).minimize(m,maxiter=500)\n",
    "# #     ScipyOptimizer().minimize(m,maxiter=4000)\n",
    "# Xs = XT\n",
    "# S = 10\n",
    "# mean,var= m.predict_y(Xs,S)\n",
    "# fmean = np.average(mean,0)\n",
    "# fvar = np.average(var,0)\n",
    "# #     temp2 = np.average(np.absolute((np.round(fmean) - YT)))\n",
    "# #     tempInc = temp1 - temp2\n",
    "# #     temp1 = temp2\n",
    "# print(testString + str(j))\n",
    "# print(np.average(np.absolute((np.round(fmean) - YT))))\n",
    "# #     i = i +1 \n",
    "# #     print(i )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ULSTXFWGCacj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DGP1 100          Mean Absolute Error : 0.5805 +- 0.0051\n",
      "DGP2 100          Mean Absolute Error : 0.5836 +- 0.0047\n",
      "DGP3 100          Mean Absolute Error : 0.5839 +- 0.0046\n",
      "DGP4 100          Mean Absolute Error : 0.5847 +- 0.0049\n"
     ]
    }
   ],
   "source": [
    "meanMAErr = np.mean(MAErr,0)\n",
    "stdMAErr = np.std(MAErr,0)\n",
    "\n",
    "for i in range(len(names_dgp)):\n",
    "    print(s.format(names_dgp[i],meanMAErr[i],stdMAErr[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X_qk0wnuDWH4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "DGP_Ordinal_Regression_Abalone.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
